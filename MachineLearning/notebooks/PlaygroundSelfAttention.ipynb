{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The mathematical trick in self-attention \n",
    "\n",
    "# considering the following toy example \n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # Batch, time, channels\n",
    "x = torch.randn(B,T,C) # (Batch, Time, Channel)\n",
    "x.shape \n",
    "# torch.Size([4,8,2])\n",
    "\n",
    "# If we have 8 tokens, we want the token at 5tyh location to communicate with \n",
    "# those in 1,2,3,4 location, but not 6,7,8 because they are in the future.\n",
    "# The information only flows from the previous context to the current timestamp,\n",
    "# We do not want to use the future tokens because we want to predict them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the very weak form of gathering the info, but for now ok\n",
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # x Bag of words, term used when you just averaging a bunch of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first one are equal, but the folowing are the averages of all the tokens before the last i\n",
    "xbow[0]\n",
    "# Nevermind, this is not very efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower triangular portion \n",
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= \n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b= \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--------------------\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# Toy example of doing it using matrix multiplication\n",
    "torch.manual_seed(42)\n",
    "# a = torch.ones(3,3)\n",
    "# Basically here we do a sum over all the rows depending on the ones we have. \n",
    "# Very efficient method\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "# But we can also do average, so we can weigth each element\n",
    "a = a / torch.sum(a, 1, keepdim=True) # a in a oneth dimention\n",
    "# So we can do avergaes in the incremental way\n",
    "b = torch.randint(0,10, (3,2)).float()\n",
    "c = a @ b \n",
    "\n",
    "print('a= ')\n",
    "print(a)\n",
    "print('b= ')\n",
    "print(b)\n",
    "print('-'*20)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Mean: True\n",
      "Using Softmax: True\n"
     ]
    }
   ],
   "source": [
    "# This is the very weak form of gathering the info, but for now ok\n",
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "torch.manual_seed(666)\n",
    "B,T,C = 4,8,2 # Batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "xbow = torch.zeros((B,T,C)) # x Bag of words, term used when you just averaging a bunch of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "        \n",
    "# Version of matrices ------------------------------------------------------\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "# wei ## Obtaining the weights\n",
    "xbow2 = wei @ x # (T,T) @ (B, T, C) ---> (B,T,T) @ (B, T, C) ---> (B,T,C)\n",
    "print(f\"Using Mean: {torch.allclose(xbow, xbow2)}\")\n",
    "\n",
    "# Version using Softmax ------------------------------------------------------\n",
    "# BASIC PREVIEW FOR THE SELF ATTENTION, WEIGHTET AGGREGATION OF YOUR PAST \n",
    "# ELEMENTS BY USING MATRIX MULTIPLICATION OF ALL THE PAST TOKENS, SO IT'S TELLING US\n",
    "# HOW MUCH OF EACH ELEMENT FUZZES IN THE EACH POSITION\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# This is more interesting because all the weights becomes zero, like infinity\n",
    "# Says us how much tokens from the past we want to aggregate in our job\n",
    "# These tokens are data dependent, these tokens will start looking at each other, \n",
    "# They will find other tokens more os less interesting\n",
    "wei = torch.zeros((T,T))\n",
    "# By setting them to negative infinity we basically saying that we will not aggregate \n",
    "# anithing from these tokens\n",
    "# Basically here we are saying that the future can not communicate with the past\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # all the 0s in the tril, becomes infinity\n",
    "# Here basically we will aggregate its values on how interesting are each other to every token \n",
    "wei = F.softmax(wei, dim=1) # Softmax in all the rows\n",
    "xbow3 = wei @ x \n",
    "torch.allclose(xbow, xbow3)\n",
    "print(f\"Using Softmax: {torch.allclose(xbow, xbow3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2]) torch.Size([4, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True,  True],\n",
       "         [ True,  True],\n",
       "         [ True, False],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [False, False],\n",
       "         [False, False],\n",
       "         [ True,  True]],\n",
       "\n",
       "        [[ True,  True],\n",
       "         [ True,  True],\n",
       "         [False, False],\n",
       "         [ True,  True],\n",
       "         [False,  True],\n",
       "         [False,  True],\n",
       "         [False, False],\n",
       "         [ True, False]],\n",
       "\n",
       "        [[ True,  True],\n",
       "         [ True,  True],\n",
       "         [False,  True],\n",
       "         [ True,  True],\n",
       "         [False,  True],\n",
       "         [ True, False],\n",
       "         [ True, False],\n",
       "         [False,  True]],\n",
       "\n",
       "        [[ True,  True],\n",
       "         [ True,  True],\n",
       "         [ True, False],\n",
       "         [ True,  True],\n",
       "         [ True,  True],\n",
       "         [False, False],\n",
       "         [ True, False],\n",
       "         [ True,  True]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(xbow.shape, xbow2.shape)\n",
    "xbow == xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7747,  0.7926],\n",
       "        [-0.3905,  0.1775],\n",
       "        [-0.1051,  0.0556],\n",
       "        [-0.3032,  0.1460],\n",
       "        [-0.3193,  0.1246],\n",
       "        [-0.1850,  0.0793],\n",
       "        [-0.2222,  0.0631],\n",
       "        [-0.1917,  0.0484]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SELF ATTENTION for the singular individual head \n",
    "torch.manual_seed(666)\n",
    "B,T,C = 4,8,32 # Batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# We do not want this to be all uniform, because different tokens \n",
    "# will find other different tokens and we won't to be independent\n",
    "# I want to gather information from the past but I want to be in a data \n",
    "# dependent way and this is what self attention solves\n",
    "\n",
    "# wei = torch.zeros((T,T))\n",
    "\n",
    "# The way we get affinities between these tokens now is by using \n",
    "# a dot product between the keys and queries, so my query dot product with \n",
    "# all the keys of all the other tokens, and this dot product now becomes wei\n",
    "# So if the key and a Query are self aligned, theu will interact in very hight \n",
    "# amount and i will get to learn more about that specific token as \n",
    "# oposed to any other token in a sequence\n",
    "\n",
    "# Let's create a single HEAD perform self-attention\n",
    "head_size = 16 \n",
    "# The self Attention mechanism\n",
    "# This will just perform a matriz multiply with a some weights\n",
    "# out = inpuyt X W^t + b\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "# Just produce this staff by forwarding x to each module\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# Now the weights will be data dependent\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # If we delete this, it will \n",
    "# communicate with all the tokens, so it will be called ENCODER block, if we leave\n",
    "# this line, it will be a DECODER block, as we are making a masked trill op\n",
    "\n",
    "wei = F.softmax(wei, dim=1) # Softmax in all the rows\n",
    "# x will be like a private info for this token, x is private to this token,\n",
    "# V is what it's aggregated for the purposes of this single head, \n",
    "# values Q and K\n",
    "v = value(x)\n",
    "out = wei @ v # V is the vector that we aggregate except of the raw x\n",
    "# out = wei @ x \n",
    "\n",
    "# There is no notion of space, attencion only happens in a set of vectors\n",
    "# from the graph, no node knows where is he, so we need to encode him a position \n",
    "# \n",
    "out.shape\n",
    "\n",
    "# It is called self attention because all the keys, the queries and the values all \n",
    "# come from the same source which is x, so the same source x produces keys, \n",
    "# queries and values, so this nodes are self attended\n",
    "\n",
    "# In other cases, in encoder- decoder transformers you can have for example \n",
    "# queryes are produced in x, but then the keys and the values could be produced \n",
    "# in a all separate source, sometimes from an ENCODER blocks, that encodes\n",
    "# A CONTEXT THAT WE WANT TO CONDITION them. So ein this case we are just producing\n",
    "# queries but we rig the information from the side. So cross attention is used \n",
    "# when the is a separate source of nodes we like to pull information from \n",
    "# into our nodes. And in self-attention is when we have nodes that talk to \n",
    "# each other in the same block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1400, 1.0236, 1.1396, 0.9841, 0.6756, 0.7669, 0.8734, 0.7898],\n",
       "        [1.2575, 1.3668, 1.6682, 1.3182, 1.1020, 1.3072, 1.1687, 1.3637],\n",
       "        [0.6605, 0.8261, 0.8798, 0.9264, 0.5227, 0.5701, 0.7271, 0.7566],\n",
       "        [0.9049, 0.8630, 1.1582, 0.9967, 0.7957, 0.8470, 0.9116, 0.9257],\n",
       "        [0.9325, 1.1924, 1.3057, 1.2844, 0.8200, 1.0326, 1.1037, 1.1075],\n",
       "        [0.9765, 1.1116, 1.1894, 1.1841, 0.8487, 0.9106, 0.9587, 0.9484],\n",
       "        [0.8570, 1.0748, 1.2791, 1.1276, 0.9151, 0.9637, 1.0107, 1.1278],\n",
       "        [1.3683, 1.3410, 1.5229, 1.3319, 1.0144, 1.2033, 1.2456, 1.2996]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0822)\n",
      "tensor(0.0914)\n",
      "tensor(0.0488)\n"
     ]
    }
   ],
   "source": [
    "# We also divide the before expresion by 1/sqrt(head_size), called \n",
    "# SCALED ATTENTION\n",
    "# that's because it makes so when input Q,K are unit variance, wei will be\n",
    "# unit variance too and Softmax will stay diffuse and not saturate too much.\n",
    "# for example \n",
    "k = torch.rand(B,T,head_size)\n",
    "q = torch.rand(B,T,head_size)\n",
    "\n",
    "# So we can see that our variance could be over our hea_size, so we need\n",
    "# simply to normalize it\n",
    "wei = q @k.transpose(-2, -1) * head_size**-0.5\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())\n",
    "# print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "f1 = torch.tensor([[1,2,3]])\n",
    "f2 = torch.tensor([[3,4,5]])\n",
    "\n",
    "print(torch.cat([f1,f2], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # How many parallel sequences will we process?\n",
    "block_size = 8 # What is the max context prediction length? \n",
    "max_iters = 5000 \n",
    "n_embd = 32 # Number of embedding directions\n",
    "eval_iters = 200\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3 # Self attention can't tolerate very hight learning rates \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 2]],\n",
       "\n",
       "        [[2, 2]],\n",
       "\n",
       "        [[3, 3]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([[1,1]])\n",
    "t2 = torch.tensor([\n",
    "    [[1,1]],\n",
    "    [[1,1]],\n",
    "    [[2,2]]\n",
    "])\n",
    "t1 +t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.73"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
